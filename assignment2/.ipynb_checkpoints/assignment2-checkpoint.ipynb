{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Fudan PRML 23Spring Assignment2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>*Your Name, Student ID and Date: [Name], [Student ID], [Date]*</font>\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red' size = 3> *DDL: 2023.06.11 23:59* </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. FDUNN: your toy torch-like deep learning library (60 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will fist implement your own torch-like deep learning library with `numpy`, named `fdunn`.\n",
    "\n",
    "PyTorch: [Link](https://pytorch.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# setup code, auto reload your .py file\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# numpy\n",
    "import numpy as np\n",
    "np.random.seed(233)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# matplotlib\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to impletement several standard deep neural modelwork modules in the `./fdunn` folder:\n",
    "1.   linear/conv/pooling\n",
    "2.   activation\n",
    "3.   loss\n",
    "4.   optim\n",
    "5.   trainer\n",
    "\n",
    "We have written most of the code for you already, and you only need to fill in the most essential parts. We have also prepared several test cases for you to check if your code works correctly.\n",
    "\n",
    "Furthermore, you can also test the accuracy of your code by comparing its output with the output of sk-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'C:\\Users\\Life_Dancer\\Desktop\\PRML-Spring23-FDU\\assignment2\\fdunn')\n",
    "\n",
    "from fdunn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fdunn.modules.linear import Linear\n",
    "from fdunn.modules.activation import Sigmoid\n",
    "from fdunn.modules.loss import BCELoss, CrossEntropyLoss\n",
    "from fdunn.optim.sgd import SGD\n",
    "\n",
    "class FNN:\n",
    "    def __init__(self, in_features, hidden_size, num_classes):\n",
    "        self.linear1 = Linear(in_features, hidden_size)\n",
    "        self.sigmoid = Sigmoid()\n",
    "        self.linear2 = Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = self.linear1.forward(input)\n",
    "        output = self.sigmoid.forward(output)\n",
    "        output = self.linear2.forward(output)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, output_grad):\n",
    "        output_grad = self.linear2.backward(output_grad)\n",
    "        output_grad = self.sigmoid.backward(output_grad)\n",
    "        output_grad = self.linear1.backward(output_grad)\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FNN(3072, 256, 10)\n",
    "criterion = CrossEntropyLoss(model)\n",
    "\n",
    "lr = 0.01\n",
    "optimizer = SGD(model, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ad94d03063445c7b4c9d693700c5158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of fdunn.data.data_loader failed: Traceback (most recent call last):\n",
      "  File \"e:\\Anaconda3\\envs\\d2l\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 257, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"e:\\Anaconda3\\envs\\d2l\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 455, in superreload\n",
      "    module = reload(module)\n",
      "  File \"e:\\Anaconda3\\envs\\d2l\\lib\\importlib\\__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 843, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"c:\\Users\\Life_Dancer\\Desktop\\PRML\\assignment2\\fdunn\\data\\data_loader.py\", line 34, in <module>\n",
      "    trainloader, testloader = get_dataset('CIFAR10')\n",
      "  File \"c:\\Users\\Life_Dancer\\Desktop\\PRML\\assignment2\\fdunn\\data\\data_loader.py\", line 23, in get_dataset\n",
      "    dst_train = datasets.CIFAR10(data_folder, train=True, download=True, transform=transform) # no augmentation\n",
      "  File \"e:\\Anaconda3\\envs\\d2l\\lib\\site-packages\\torchvision\\datasets\\cifar.py\", line 65, in __init__\n",
      "    self.download()\n",
      "  File \"e:\\Anaconda3\\envs\\d2l\\lib\\site-packages\\torchvision\\datasets\\cifar.py\", line 141, in download\n",
      "    download_and_extract_archive(self.url, self.root, filename=self.filename, md5=self.tgz_md5)\n",
      "KeyboardInterrupt\n",
      "]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cifar/trainloader.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Life_Dancer\\Desktop\\PRML\\assignment2\\assignment2.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Life_Dancer/Desktop/PRML/assignment2/assignment2.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpickle\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Life_Dancer/Desktop/PRML/assignment2/assignment2.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mcifar/trainloader.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Life_Dancer/Desktop/PRML/assignment2/assignment2.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     trainloader, testloader \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(f)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cifar/trainloader.pkl'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('cifar/trainloader.pkl', 'rb') as f:\n",
    "    trainloader, testloader = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b508dff10c60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mfdunn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'CIFAR10'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'CIFAR10'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\PRML\\assignment2\\fdunn\\data\\data_loader.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "from fdunn.data.data_loader import get_dataset\n",
    "\n",
    "dataset = 'CIFAR10'\n",
    "trainloader, testloader = get_dataset('CIFAR10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch(mode, dataloader, model, optimizer, criterion):\n",
    "    loss_avg, acc_avg, num_exp = 0, 0, 0\n",
    "\n",
    "    for i_batch, datum in enumerate(dataloader):\n",
    "        img = datum[0]\n",
    "        lab = datum[1]\n",
    "\n",
    "        n_b = lab.shape[0]\n",
    "\n",
    "        output = model.forward(img)\n",
    "        # print(output)\n",
    "        loss = criterion.forward(output, lab)\n",
    "\n",
    "        predicted = np.argmax(output, 1)\n",
    "        correct = (predicted == lab).sum()\n",
    "\n",
    "        loss_avg += loss.item()*n_b\n",
    "        acc_avg += correct.item()\n",
    "        num_exp += n_b\n",
    "\n",
    "        if mode == 'train':\n",
    "            # optimizer.zero_grad() should we implement zero_grad()?\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    loss_avg /= num_exp\n",
    "    acc_avg /= num_exp\n",
    "\n",
    "    return loss_avg, acc_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(sync_tensorboard=False,\n",
    "        entity='tongchen',\n",
    "        project=\"PRML\",\n",
    "        name='{}-{}-{}-{}'.format(dataset, model, lr, criterion)\n",
    "    )\n",
    "\n",
    "train_epochs = 20\n",
    "lr_schedule = [10, 15]\n",
    "decay = True\n",
    "\n",
    "for e in range(train_epochs):\n",
    "\n",
    "    train_loss, train_acc = epoch(\"train\", dataloader=trainloader, model=model, optimizer=optimizer, criterion=criterion)\n",
    "\n",
    "    test_loss, test_acc = epoch(\"test\", dataloader=testloader, model=model, optimizer=None, criterion=criterion)\n",
    "\n",
    "    print(\"Epoch: {}\\tReal Acc: {}\\tTest Acc: {}\".format(e, train_acc, test_acc))\n",
    "\n",
    "    wandb.log({'Train Loss': train_loss}, step=e)\n",
    "    wandb.log({'Train Acc': train_acc}, step=e)\n",
    "    wandb.log({'Test Loss': test_loss}, step=e)\n",
    "    wandb.log({'Test Acc': test_acc}, step=e)\n",
    "    wandb.log({'lr': lr}, step=e)\n",
    "\n",
    "    if e in lr_schedule and decay:\n",
    "        lr *= 0.1\n",
    "        optimizer = SGD(model, lr=lr)\n",
    "        # optimizer.zero_grad()   should we implement zero_grad()?\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "44bc79bdedc951859a4620ce4f5a740abfa1bf27d20da6af0e4010c444744bef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
